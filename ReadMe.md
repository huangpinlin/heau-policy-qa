export const filesData = [
    {
        name: 'README.md',
        description: 'Project overview, technical architecture, setup, and usage guide.',
        content: `# æ²³å—å¤§å­¦æ”¿ç­–é—®ç­”ç³»ç»Ÿ

ä¸€ä¸ªåŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œä¸ºæ²³å—å¤§å­¦å¸ˆç”Ÿæä¾›ç²¾å‡†ã€å¯æº¯æºçš„æ”¿ç­–æŸ¥è¯¢æœåŠ¡ã€‚

![æ²³å—å¤§å­¦æ”¿ç­–é—®ç­”ç³»ç»Ÿç•Œé¢](https://r2.flowith.net/files/85b35d1e-d6a4-458b-ab2b-2396aae76f0a/1754543336217-%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE_2025-08-07_130813@1340x660.png)

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

-   **è‡ªç„¶è¯­è¨€ç†è§£**ï¼šç”¨æˆ·å¯ä»¥ä½¿ç”¨æ—¥å¸¸å¯¹è¯æ–¹å¼è¿›è¡Œæé—®ï¼Œæ— éœ€å…³å¿ƒå…³é”®è¯æˆ–ä¸“ä¸šæœ¯è¯­ã€‚
-   **ç²¾å‡†ä¿¡æ¯æ£€ç´¢**ï¼šåŸºäºå‘é‡åŒ–æ£€ç´¢æŠ€æœ¯ï¼Œå¿«é€Ÿä»æµ·é‡æ”¿ç­–æ–‡ä»¶ä¸­å®šä½æœ€ç›¸å…³çš„æ¡æ¬¾å’Œæ®µè½ã€‚
-   **æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆ**ï¼šç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå°†æ£€ç´¢åˆ°çš„ä¿¡æ¯æ•´åˆæˆé€šé¡ºã€æ˜“æ‡‚çš„å›ç­”ã€‚
-   **ç­”æ¡ˆæ¥æºå¯è¿½æº¯**ï¼šæ¯ä¸ªå›ç­”éƒ½ä¼šé™„ä¸Šæ¥æºæ–‡æ¡£çš„æ ‡é¢˜ã€é“¾æ¥å’ŒåŸæ–‡ç‰‡æ®µï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¸å®ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚
-   **å‹å¥½äº¤äº’ç•Œé¢**ï¼šæä¾›åŸºäº Streamlit çš„ Web ç•Œé¢ï¼Œæ“ä½œç›´è§‚ï¼Œä¸Šæ‰‹ç®€å•ã€‚

## ğŸ› ï¸ æŠ€æœ¯æ¶æ„

æœ¬ç³»ç»Ÿé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¶æ„ï¼Œç»“åˆäº†å¤šç§å…ˆè¿›çš„AIæŠ€æœ¯å’Œæ¡†æ¶ï¼Œä»¥å®ç°é«˜æ•ˆã€å‡†ç¡®çš„é—®ç­”åŠŸèƒ½ã€‚**ç³»ç»Ÿæœ€è¿‘å·²é‡æ„ï¼Œå°†åµŒå…¥æ¨¡å—ä»\`sentence-transformers\`è¿ç§»è‡³\`fastembed\`ï¼Œä»¥æå‡æ€§èƒ½å’Œç®€åŒ–ä¾èµ–ã€‚**

| æŠ€æœ¯/ç»„ä»¶ | ç”¨é€”è¯´æ˜ |
| :--- | :--- |
| **LangChain** | ä½œä¸ºæ ¸å¿ƒç¼–æ’æ¡†æ¶ï¼Œæ•´åˆäº†æ•°æ®å¤„ç†ã€æ¨¡å‹è°ƒç”¨ã€æ£€ç´¢å’Œé—®ç­”é“¾ï¼ˆQA Chainï¼‰çš„å…¨éƒ¨æµç¨‹ã€‚ |
| **Qwen-7B-Chat** | é˜¿é‡Œå·´å·´é€šä¹‰åƒé—®70äº¿å‚æ•°å¯¹è¯æ¨¡å‹ï¼Œä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè´Ÿè´£ç†è§£é—®é¢˜å¹¶åŸºäºæ£€ç´¢å†…å®¹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ |
| **fastembed (BAAI/bge-base-en-v1.5)** | ä¸€ä¸ªè½»é‡çº§ã€é«˜æ€§èƒ½çš„åµŒå…¥æ¨¡å‹åº“ã€‚é€šè¿‡\`ONNX Runtime\`ä¼˜åŒ–ï¼Œæ›¿ä»£äº†åŸæœ‰çš„\`sentence-transformers\`æ–¹æ¡ˆï¼Œå®ç°äº†æ›´å¿«çš„å‘é‡åŒ–é€Ÿåº¦å’Œæ›´ä½çš„èµ„æºå ç”¨ï¼Œå°¤å…¶åœ¨CPUä¸Šè¡¨ç°å‡ºè‰²ã€‚ |
| **FAISS (Facebook AI Similarity Search)** | é«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢å¼•æ“ï¼Œç”¨äºå­˜å‚¨æ–‡æœ¬å‘é‡å¹¶å®ç°å¿«é€Ÿæ£€ç´¢ã€‚ |
| **Streamlit** | ä¸€ä¸ªå¼€æºçš„Pythonæ¡†æ¶ï¼Œç”¨äºå¿«é€Ÿæ„å»ºå’Œéƒ¨ç½²æ•°æ®ç§‘å­¦ä¸æœºå™¨å­¦ä¹ çš„Webåº”ç”¨ï¼Œæ˜¯æœ¬ç³»ç»Ÿçš„ç”¨æˆ·ç•Œé¢ã€‚ |

\`\`\`mermaid
flowchart LR
    A[ç”¨æˆ·åœ¨Streamlitç•Œé¢è¾“å…¥é—®é¢˜] --> B{LangChain QA Chain};
    B --> C[ä½¿ç”¨fastembedå°†é—®é¢˜å‘é‡åŒ–];
    C --> D[åœ¨FAISSç´¢å¼•ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£];
    D --> E{Promptæ„å»º};
    E -- æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ --> F[Qwen-7B LLM];
    E -- åŸå§‹é—®é¢˜ --> F;
    F --> G[ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ];
    G --> H[åœ¨Streamlitç•Œé¢å±•ç¤ºç­”æ¡ˆåŠæ¥æº];
\`\`\`

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

-   **Python ç‰ˆæœ¬**: æ¨è \`Python 3.8\` ~ \`3.11\`
-   **ç¡¬ä»¶è¦æ±‚**:
    -   **å¼ºçƒˆæ¨è**ï¼šé…å¤‡è‡³å°‘ 16GBæ˜¾å­˜çš„ NVIDIA GPUï¼Œä»¥ä¿è¯ \`Qwen-7B\` æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ã€‚
    -   **æœ€ä½é…ç½®**ï¼šå¯åœ¨ CPU ä¸Šè¿è¡Œã€‚å¾—ç›Šäº\`fastembed\`ï¼Œç´¢å¼•æ„å»ºæ­¥éª¤åœ¨CPUä¸Šä¾ç„¶é«˜æ•ˆã€‚ä½†LLMæ¨ç†é€Ÿåº¦ä¼šéå¸¸æ…¢ã€‚
-   **ä¾èµ–åº“**: æ‰€æœ‰ä¾èµ–é¡¹å‡å·²åœ¨ \`requirements.txt\` æ–‡ä»¶ä¸­åˆ—å‡ºã€‚

    \`\`\`
    streamlit==1.35.0
    langchain==0.1.16
    langchain-community==0.0.34
    faiss-cpu==1.8.0
    fastembed>=0.2.0
    onnxruntime>=1.16.0
    transformers==4.40.1
    torch==2.2.1
    accelerate==0.29.3
    numpy==1.26.4
    tqdm==4.66.4
    \`\`\`

## ğŸš€ å¿«é€Ÿå¼€å§‹

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åœ¨æ‚¨çš„æœ¬åœ°ç¯å¢ƒä¸­éƒ¨ç½²å¹¶å¯åŠ¨ç³»ç»Ÿã€‚

### 1. ç¯å¢ƒå‡†å¤‡

å…‹éš†æˆ–ä¸‹è½½æœ¬é¡¹ç›®ï¼Œå¹¶åˆ›å»ºæ‰€éœ€çš„ç›®å½•ç»“æ„ã€‚

\`\`\`bash
# å‡è®¾é¡¹ç›®æ ¹ç›®å½•ä¸º henan_university_qa
mkdir -p henan_university_qa/data henan_university_qa/faiss_index
cd henan_university_qa
\`\`\`

### 2. ä¾èµ–å®‰è£…

ä½¿ç”¨ \`pip\` å®‰è£…æ‰€æœ‰å¿…éœ€çš„ Python åº“ã€‚

\`\`\`bash
pip install -r requirements.txt
\`\`\`

### 3. æ•°æ®å‡†å¤‡

å°†åŒ…å«æ”¿ç­–æ–‡æœ¬çš„çŸ¥è¯†åº“æ–‡ä»¶ \`processed_policies.json\` æ”¾ç½®åˆ° \`data/\` ç›®å½•ä¸‹ã€‚

> é¡¹ç›®ç»“æ„åº”å¦‚ä¸‹æ‰€ç¤ºï¼š
> \`\`\`
> /henan_university_qa/
> â”œâ”€â”€ data/
> â”‚   â””â”€â”€ processed_policies.json
> â”œâ”€â”€ faiss_index/
> â”œâ”€â”€ app.py
> â”œâ”€â”€ create_index.py
> â”œâ”€â”€ qa_system_core.py
> â””â”€â”€ requirements.txt
> \`\`\`

### 4. æ„å»ºçŸ¥è¯†åº“ç´¢å¼•

æ­¤æ­¥éª¤ä¼šå°† \`data/\` ç›®å½•ä¸‹çš„æ”¿ç­–æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶æ„å»ºä¸€ä¸ª FAISS ç´¢å¼•ç”¨äºå¿«é€Ÿæ£€ç´¢ã€‚

\`\`\`bash
python create_index.py
\`\`\`
> **æ³¨æ„**ï¼šé¦–æ¬¡è¿è¡Œæ­¤è„šæœ¬ä¼šç”±\`fastembed\`è‡ªåŠ¨ä¸‹è½½åµŒå…¥æ¨¡å‹ (\`BAAI/bge-base-en-v1.5\`)ã€‚æ­¤è¿‡ç¨‹å·²ä¼˜åŒ–ï¼Œä¸‹è½½å’Œå¤„ç†é€Ÿåº¦è¾ƒå¿«ã€‚æˆåŠŸåï¼Œ\`faiss_index/\` ç›®å½•ä¸‹ä¼šç”Ÿæˆ \`faiss_index.bin\` å’Œ \`documents_metadata.json\` ä¸¤ä¸ªæ–‡ä»¶ã€‚

### 5. å¯åŠ¨ç³»ç»Ÿ

è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Streamlit Web åº”ç”¨ã€‚

\`\`\`bash
streamlit run app.py
\`\`\`

å¯åŠ¨æˆåŠŸåï¼Œç»ˆç«¯ä¼šæ˜¾ç¤ºæœ¬åœ°è®¿é—®åœ°å€ï¼ˆé€šå¸¸æ˜¯ \`http://localhost:8501\`ï¼‰ï¼Œåœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ­¤åœ°å€å³å¯å¼€å§‹ä½¿ç”¨ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

\`\`\`
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed_policies.json   # é¢„å¤„ç†åçš„æ”¿ç­–çŸ¥è¯†åº“æºæ–‡ä»¶
â”œâ”€â”€ faiss_index/
â”‚   â”œâ”€â”€ faiss_index.bin           # (è‡ªåŠ¨ç”Ÿæˆ) FAISS å‘é‡ç´¢å¼•æ–‡ä»¶
â”‚   â””â”€â”€ documents_metadata.json   # (è‡ªåŠ¨ç”Ÿæˆ) ä¸ç´¢å¼•å¯¹åº”çš„æ–‡æ¡£å…ƒæ•°æ®
â”œâ”€â”€ app.py                      # Streamlit Web åº”ç”¨ä¸»ç¨‹åº
â”œâ”€â”€ create_index.py             # ç”¨äºæ„å»º FAISS ç´¢å¼•çš„è„šæœ¬
â”œâ”€â”€ qa_system_core.py           # å°è£…äº†RAGæ ¸å¿ƒé€»è¾‘çš„æ¨¡å—
â””â”€â”€ requirements.txt            # Python ä¾èµ–é¡¹åˆ—è¡¨
\`\`\`

## ğŸ’¬ ä½¿ç”¨è¯´æ˜

1.  åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ \`http://localhost:8501\`ã€‚
2.  åœ¨ä¸»ç•Œé¢çš„æ–‡æœ¬è¾“å…¥æ¡†ä¸­ï¼Œè¾“å…¥æ‚¨æƒ³å’¨è¯¢çš„å…³äºå­¦æ ¡æ”¿ç­–çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼šâ€œWhat are the rewards for undergraduate research projects?â€
3.  ç‚¹å‡»â€œæäº¤é—®é¢˜â€æŒ‰é’®ã€‚
4.  ç³»ç»Ÿä¼šè¿›è¡Œæ€è€ƒï¼Œå¹¶åœ¨ä¸‹æ–¹æ˜¾ç¤º AI ç”Ÿæˆçš„å›ç­”ã€‚
5.  æ‚¨å¯ä»¥å±•å¼€â€œæŸ¥çœ‹ç­”æ¡ˆæ¥æºâ€éƒ¨åˆ†ï¼Œæ ¸å¯¹ç­”æ¡ˆæ‰€ä¾æ®çš„åŸå§‹æ”¿ç­–æ–‡ä»¶å’Œç›¸å…³ç‰‡æ®µã€‚

## âš™ï¸ é…ç½®è¯´æ˜

### è‡ªå®šä¹‰æ¨¡å‹å’Œå‚æ•°

ç³»ç»Ÿçš„æ ¸å¿ƒé…ç½®ä½äº \`qa_system_core.py\` æ–‡ä»¶ä¸­çš„ \`QAConfig\` ç±»ã€‚æ‚¨å¯ä»¥ç›´æ¥ä¿®æ”¹æ­¤ç±»ä¸­çš„å‚æ•°æ¥è‡ªå®šä¹‰ç³»ç»Ÿè¡Œä¸ºã€‚

\`\`\`python
# qa_system_core.py

class QAConfig:
    """ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿæ‰€éœ€çš„æ‰€æœ‰é…ç½®å‚æ•°ã€‚"""
    FAISS_INDEX_PATH = 'faiss_index/faiss_index.bin'
    METADATA_PATH = 'faiss_index/documents_metadata.json'
    # åµŒå…¥æ¨¡å‹å·²æ›´æ–°ä¸ºfastembedæ”¯æŒçš„è½»é‡çº§æ¨¡å‹
    EMBEDDING_MODEL_NAME = 'BAAI/bge-base-en-v1.5'  # å¯æ›´æ¢ä¸ºå…¶ä»–fastembedæ”¯æŒçš„æ¨¡å‹
    LLM_MODEL_PATH = 'Qwen/Qwen-7B-Chat'             # å¯æ›´æ¢ä¸ºå…¶ä»–LLM
    SEARCH_TOP_K = 4                                # è°ƒæ•´æ£€ç´¢æ–‡æ¡£çš„æ•°é‡
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
\`\`\`

### æ›´æ–°çŸ¥è¯†åº“

1.  **å‡†å¤‡æ–°æ•°æ®**ï¼šå°†æ–°çš„æ”¿ç­–æ–‡æœ¬å¤„ç†æˆä¸ \`processed_policies.json\` ç›¸åŒæ ¼å¼çš„ JSON æ–‡ä»¶ï¼Œå¹¶æ›¿æ¢æ‰æ—§æ–‡ä»¶ã€‚
2.  **åˆ é™¤æ—§ç´¢å¼•**ï¼šåˆ é™¤ \`faiss_index/\` ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ã€‚
3.  **é‡å»ºç´¢å¼•**ï¼šé‡æ–°è¿è¡Œ \`create_index.py\` è„šæœ¬ï¼Œä»¥æ ¹æ®æ–°æ•°æ®ç”Ÿæˆæ–°çš„å‘é‡ç´¢å¼•ã€‚

    \`\`\`bash
    python create_index.py
    \`\`\`

4.  **é‡å¯åº”ç”¨**ï¼šé‡å¯ Streamlit åº”ç”¨å³å¯åŠ è½½æ–°çš„çŸ¥è¯†åº“ã€‚

## â“ å¸¸è§é—®é¢˜ (FAQ)

1.  **Q: å¯åŠ¨ \`app.py\` æ—¶ä¸ºä»€ä¹ˆéå¸¸ç¼“æ…¢æˆ–å¡ä½ï¼Ÿ**
    *   **A:** é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œç³»ç»Ÿéœ€è¦ä» Hugging Face Hub ä¸‹è½½å¹¶åŠ è½½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ \`Qwen-7B-Chat\`ï¼‰åˆ°å†…å­˜/æ˜¾å­˜ä¸­ã€‚è¿™ä¸ªè¿‡ç¨‹æ ¹æ®æ‚¨çš„ç½‘ç»œå’Œç¡¬ä»¶æƒ…å†µï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿã€‚è¯·è€å¿ƒç­‰å¾…ï¼Œåç»­å¯åŠ¨ä¼šå¿«å¾—å¤šï¼ˆå› ä¸ºæ¨¡å‹å·²æœ‰ç¼“å­˜ï¼‰ã€‚åµŒå…¥æ¨¡å‹çš„ä¸‹è½½ç”±\`fastembed\`å¤„ç†ï¼Œé€šå¸¸éå¸¸è¿…é€Ÿã€‚

2.  **Q: è¿è¡Œæ—¶å‡ºç° \`FileNotFoundError\`ï¼Œæç¤ºæ‰¾ä¸åˆ° \`faiss_index.bin\` æ–‡ä»¶ã€‚**
    *   **A:** è¿™ä¸ªé”™è¯¯è¡¨ç¤ºæ‚¨å°šæœªæˆåŠŸæ„å»ºçŸ¥è¯†åº“ç´¢å¼•ã€‚è¯·ç¡®ä¿æ‚¨å·²ç»æˆåŠŸè¿è¡Œäº† \`python create_index.py\` è„šæœ¬ï¼Œå¹¶ä¸” \`faiss_index\` ç›®å½•ä¸‹å·²ç”Ÿæˆäº† \`.bin\` å’Œ \`.json\` æ–‡ä»¶ã€‚

3.  **Q: å¯ä»¥åœ¨æ²¡æœ‰GPUçš„ç”µè„‘ä¸Šè¿è¡Œå—ï¼Ÿ**
    *   **A:** å¯ä»¥ã€‚ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹æ˜¯å¦å­˜åœ¨ CUDA è®¾å¤‡ï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä¼šåˆ‡æ¢åˆ° CPU æ¨¡å¼ã€‚åœ¨æ–°çš„æ¶æ„ä¸‹ï¼Œä½¿ç”¨\`fastembed\`è¿›è¡Œç´¢å¼•æ„å»ºå’Œ embedding åœ¨CPUä¸Šéå¸¸é«˜æ•ˆã€‚ç„¶è€Œï¼Œè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†éƒ¨åˆ†åœ¨CPUä¸Šä»ç„¶ä¼š**æå…¶ç¼“æ…¢**ï¼Œå¯èƒ½æ— æ³•è·å¾—æµç•…çš„é—®ç­”ä½“éªŒã€‚

## ğŸ“ˆ ç»´æŠ¤å’Œæ‰©å±•

-   **æ•°æ®è‡ªåŠ¨åŒ–æ›´æ–°**: æœªæ¥å¯ä»¥å¼€å‘ç½‘ç»œçˆ¬è™«ï¼Œå®šæœŸä»æ²³å—å¤§å­¦å®˜ç½‘æŠ“å–æœ€æ–°çš„æ”¿ç­–æ–‡ä»¶ï¼Œå®ç°çŸ¥è¯†åº“çš„è‡ªåŠ¨åŒ–æ›´æ–°ã€‚
-   **ç”¨æˆ·åé¦ˆæœºåˆ¶**: åœ¨Webç•Œé¢ä¸­å¢åŠ â€œèµ/è¸©â€æˆ–è¯„åˆ†åŠŸèƒ½ï¼Œæ”¶é›†ç”¨æˆ·å¯¹ç­”æ¡ˆè´¨é‡çš„åé¦ˆï¼Œç”¨äºè¿­ä»£ä¼˜åŒ–æ£€ç´¢ç­–ç•¥å’ŒPromptæ¨¡æ¿ã€‚
-   **æ¨¡å‹å‡çº§**: éšç€AIæŠ€æœ¯çš„å‘å±•ï¼Œå¯ä»¥è½»æ¾åœ°åœ¨ \`QAConfig\` ä¸­æ›¿æ¢ä¸ºæ›´å…ˆè¿›ã€æ›´é«˜æ•ˆçš„åµŒå…¥æ¨¡å‹æˆ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](https://opensource.org/licenses/MIT) å¼€æºã€‚

## ğŸ™Œ è´¡çŒ®æŒ‡å—

æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼å¦‚æœæ‚¨æœ‰ä»»ä½•å»ºè®®æˆ–å‘ç°äº†é—®é¢˜ï¼Œè¯·éšæ—¶æäº¤ Issues æˆ– Pull Requestsã€‚`
    },
    {
        name: 'requirements.txt',
        description: 'A list of all Python dependencies required to run the project.',
        content: `streamlit==1.35.0
langchain==0.1.16
langchain-community==0.0.34
faiss-cpu==1.8.0
fastembed>=0.2.0
onnxruntime>=1.16.0
transformers==4.40.1
torch==2.2.1
accelerate==0.29.3
numpy==1.26.4
tqdm==4.66.4`
    },
    {
        name: 'create_index.py',
        description: 'Script to generate vector embeddings and build the FAISS index from source data.',
        content: `import json
import os
import faiss
import numpy as np
from fastembed import TextEmbedding
from tqdm import tqdm

# --- Constants ---
INPUT_JSON_PATH = 'data/processed_policies.json'
FAISS_INDEX_DIR = 'faiss_index'
FAISS_INDEX_PATH = os.path.join(FAISS_INDEX_DIR, 'faiss_index.bin')
METADATA_PATH = os.path.join(FAISS_INDEX_DIR, 'documents_metadata.json')
# The model is updated according to the refactoring requirements.
MODEL_NAME = 'BAAI/bge-base-en-v1.5'

def load_and_filter_data(file_path: str) -> list:
    """
    Loads and filters documents from a JSON file.
    Only documents with non-empty 'full_cleaned_text' are kept.
    """
    print(f"æ­£åœ¨ä» {file_path} åŠ è½½æ•°æ®...")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ {file_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿æ•°æ®å·²æ”¾ç½®åœ¨ 'data/' ç›®å½•ä¸‹ã€‚")
        
    with open(file_path, 'r', encoding='utf-8') as f:
        all_documents = json.load(f)

    processed_docs = [doc for doc in all_documents if doc.get('full_cleaned_text', '').strip()]
    
    print(f"åŠ è½½å®Œæˆã€‚å…±æ‰¾åˆ° {len(all_documents)} æ¡è®°å½•ï¼Œå…¶ä¸­æœ‰æ•ˆè®°å½• {len(processed_docs)} æ¡ã€‚")
    return processed_docs

def generate_embeddings(texts: list, model_name: str) -> np.ndarray:
    """
    Generates vector embeddings for a list of texts using the specified fastembed model.
    This version is lightweight and does not depend on PyTorch or Transformers.
    """
    print(f"æ­£åœ¨åŠ è½½è½»é‡çº§åµŒå…¥æ¨¡å‹: {model_name}...")
    # fastembed automatically handles device selection (CPU/GPU) via ONNX Runtime.
    # It's generally faster on CPU than sentence-transformers.
    embedding_model = TextEmbedding(model_name=model_name, cache_dir=os.getenv("HF_HOME"))
    
    print("å¼€å§‹ç”Ÿæˆæ–‡æœ¬å‘é‡åµŒå…¥...")
    # The \`embed\` method returns a generator of numpy arrays.
    embeddings_list = embedding_model.embed(texts, show_progress_bar=True)
    
    # Convert the generator of arrays into a single, stacked numpy matrix.
    embeddings = np.array(list(embeddings_list), dtype='float32')
    
    # --- CRITICAL STEP: Manual L2 Normalization ---
    # This step is essential to replicate the behavior of \`normalize_embeddings=True\`
    # from the previous SentenceTransformer implementation. BGE models are designed
    # to be used with normalized embeddings for accurate similarity search.
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    # To prevent division by zero for potential zero-vectors
    norms[norms == 0] = 1e-12
    normalized_embeddings = embeddings / norms
    
    print("å‘é‡åµŒå…¥ç”Ÿæˆå®Œæ¯•ã€‚")
    
    # FAISS requires float32 type.
    return normalized_embeddings.astype('float32')

def build_and_save_faiss_index(embeddings: np.ndarray, index_path: str):
    """
    Builds a FAISS index from embeddings and saves it to disk.
    """
    if embeddings.shape[0] == 0:
        print("æ²¡æœ‰å¯ç”¨äºæ„å»ºç´¢å¼•çš„å‘é‡ã€‚")
        return

    dimension = embeddings.shape[1]
    print(f"æ­£åœ¨æ„å»º FAISS ç´¢å¼• (IndexFlatL2)ï¼Œå‘é‡ç»´åº¦: {dimension}...")
    # IndexFlatL2 is a simple index that performs exhaustive search, suitable for many use cases.
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±æ·»åŠ äº† {index.ntotal} ä¸ªå‘é‡ã€‚")
    
    os.makedirs(os.path.dirname(index_path), exist_ok=True)
    faiss.write_index(index, index_path)
    print(f"FAISS ç´¢å¼•å·²æˆåŠŸä¿å­˜åˆ°: {index_path}")

def save_metadata(documents: list, metadata_path: str):
    """
    Saves the metadata of the indexed documents to a JSON file.
    This metadata is used to retrieve the original text content after a search.
    """
    metadata_list = []
    for doc in documents:
        metadata_list.append({
            'source_url': doc.get('source_url'),
            'title': doc.get('metadata', {}).get('title'),
            'publish_date': doc.get('metadata', {}).get('publish_date'),
            'publish_dept': doc.get('metadata', {}).get('publish_dept'),
            'document_id': doc.get('metadata', {}).get('document_id'),
            'full_cleaned_text': doc.get('full_cleaned_text')
        })
    
    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata_list, f, ensure_ascii=False, indent=4)
        
    print(f"å…ƒæ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {metadata_path}")

if __name__ == "__main__":
    try:
        valid_documents = load_and_filter_data(INPUT_JSON_PATH)
        
        if not valid_documents:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£æ¥æ„å»ºç´¢å¼•ï¼Œç¨‹åºé€€å‡ºã€‚")
        else:
            texts_to_embed = [doc['full_cleaned_text'] for doc in valid_documents]
            
            embeddings = generate_embeddings(texts_to_embed, MODEL_NAME)
            
            build_and_save_faiss_index(embeddings, FAISS_INDEX_PATH)
            
            save_metadata(valid_documents, METADATA_PATH)
            
            print("\\n--- ç´¢å¼•æ„å»ºæµç¨‹å…¨éƒ¨å®Œæˆ ---")
            
    except Exception as e:
        print(f"åœ¨æ„å»ºç´¢å¼•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
`
    },
    {
        name: 'qa_system_core.py',
        description: 'Core logic for the Q&A system, refactored to use FastEmbedEmbeddings.',
        content: `# qa_system_core_refactored.py